{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHdvG8vAs2HM"
   },
   "source": [
    "# Using Machine Learning Tools: Workshop 5\n",
    "\n",
    "The aim of this week's workshop is to get to know the task of classification and to apply and compare two different classification methods.\n",
    "\n",
    "You will use [Wisconsin Breast Cancer data set](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset), which is included in scikit learn (follow the link and read about what the dataset is)\n",
    "\n",
    "Note: the link on webpage to the dataset is 'dead'. However, you can load the data directly using the below (as used in the below code):\n",
    "\n",
    "`from sklearn.datasets import load_breast_cancer`\n",
    "\n",
    "`data = load_breast_cancer()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fPfot-jtprF"
   },
   "source": [
    "## **Setup and data loading**\n",
    "\n",
    "Import required libraries and access the Wisconsin Breast Cancer data set by running the cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Ja6I0WJs2HN"
   },
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# To plot even prettier figures\n",
    "import seaborn as sn\n",
    "\n",
    "# General data handling (pure numerics are better in numpy)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otOP5v5zs2HO"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PCqrKQMPs2HO"
   },
   "outputs": [],
   "source": [
    "# This is where the numerical data is\n",
    "xarray = data.data\n",
    "yarray = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KoO7sjiOs2HO"
   },
   "outputs": [],
   "source": [
    "# This is where the names of features and targets are\n",
    "print(f'Features names are: {data.feature_names}')\n",
    "print(f'Label names are: {data.target_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "er1wyvO0s2HP"
   },
   "outputs": [],
   "source": [
    "# We recommend inverting the labels so that malignant (the worse disease) = 1 (i.e. positive)\n",
    "yarray = 1 - yarray\n",
    "# Don't forget to switch the label names too (if you are going to use them anywhere)\n",
    "# Though it is good practice to switch them here anyway, as future modifications to the code then won't get confused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2u7ePZsHs2HP"
   },
   "outputs": [],
   "source": [
    "# This is how you could put it all into a pandas dataframe (useful for some investigations)\n",
    "df = pd.DataFrame(some_numerical_array, columns = list_of_column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_9snhv7uHPw"
   },
   "source": [
    "\n",
    "## **Inspection and visualisation [15 mins]**\n",
    "\n",
    "Familiarise yourself with the dataset and then use several different methods to display its properties, just like in Workshops 2 and 3. Pay particular attention to the class variable - this is often called the label or the target. The classifier aims to predict the label based on the remaining features in the data set. That makes it a supervised learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pULgSzUQs2HP"
   },
   "outputs": [],
   "source": [
    "# Add your code here to investigate the data further ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vvjq-0vms2HP"
   },
   "source": [
    "## **Splitting into separate datasets and building a pre-processing pipeline [10 mins]**\n",
    "\n",
    "Split the data into a train and test set using an 80:20 ratio.\n",
    "Then split the training part into a reduced training set and a validation set.  We will use a fixed hold-out validation set for this workshop, but we could have done K-fold cross-validation in the same way as we did for regression.\n",
    "\n",
    "Once this is done, build an appropriate pre-processing pipeline, based on what you’ve seen in your investigations of the data above. What elements should you put in a pre-processing pipeline and why?\n",
    "\n",
    "You can use tools like ChatGPT to help you write your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qNGtYZws2HP"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Your code for splitting into separate data sets goes here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lcc-JRle3Hym"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "preproc_pl = Pipeline([ ??? ]) # Your code for building a pre-processing pipeline goes here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fx2tkl3-s2HQ"
   },
   "source": [
    "## **Implementing a Stochastic Gradient Descent Classifier [20 mins]**\n",
    "\n",
    "Build a new pipeline that includes your preprocessing and the Stochastic Gradient Descent classifier (which is a binary classifier that uses a linear model). You can find this in `sklearn.linear_model` called `SGDClassifier`. Use the parameter setting `loss=‘log’` (as this allows us to get probability outputs, whilst other loss functions, including the default, do not have anything except binary/integer outputs).\n",
    "\n",
    "Train the classifier and then apply it to the validation set to get both binary class outputs and also probabilistic outputs using two separate calls, and store the outputs separately. We will use the binary class outputs for the next few steps, but will want the probabilities a little later.\n",
    "\n",
    "After this:\n",
    "* Display the results graphically, along with the true labels, in such a way that it is easy to identify which ones are correct or incorrect.\n",
    "* Calculate the confusion matrix using confusion_matrix from sklearn.metrics\n",
    "* Display this confusion matrix using the seaborn heatmap function, with annotations on (i.e. annot=True)\n",
    "* Calculate and display a normalised version of the confusion matrix, such that each row (True classes) sums to 1.0\n",
    "* Calculate the accuracy of the classification using accuracy_score from sklearn.metrics\n",
    "* Calculate the precision and recall using sklearn’s precision_score and recall_score\n",
    "* Calculate and display the Receiver-Operator-Characteristic (ROC) curve using roc_curve from sklearn.metrics\n",
    "* Calculate the Area Under the Curve (AUC) using the function auc from sklearn.metrics\n",
    "* Calculate and display the Precision-Recall curve (using precision_recall_curve). This plots Precision vs Recall in the same way as an ROC curve, as this is an alternative way of looking at things, but where the top right corner represents the best result. These are particularly useful when you don’t care about True Negatives, as these don’t feature in Precision or Recall.\n",
    "\n",
    "You can use tools like ChatGPT to help you write your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oek-bd4ls2HQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Rjq6P0Zx1rG"
   },
   "source": [
    "\n",
    "**Questions:**\n",
    "* How many distinct points are there in the ROC curve? Try calculating the ROC curve again using the probability outputs instead. Look at the thresholds and compare these to the predicted probability outputs from the classifier (especially those where the classification was incorrect). There is an additional notebook (ROC.ipynb) provided in the MyUni module where you can observe how ROC curve is created.\n",
    "\n",
    "* If this classifier would be used to make decisions in the hospital, which threshold would you choose? Is precision more important or recall? Do you think this classifier is good enough or does it need more optimisation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_wP7xiWs2HQ"
   },
   "source": [
    "## **Implementing a Decision Tree Classifier [20 mins]**\n",
    "\n",
    "Train a decision tree classifier on the same training data using the `DecisionTreeClassifier` from `sklearn.tree`. The default parameters are fine for this.\n",
    "\n",
    "Display the decision tree using the function `plot_tree` in `sklearn.tree`. Hint: To increase the resolution use `plt.rcParams['figure.dpi'] = 200` (if you did our standard `import matplotlib.pyplot as plt`)\n",
    "\n",
    "After this:\n",
    "* Apply the classifier to the validation set. Note that you cannot get a probability output from a single decision tree.\n",
    "* Calculate the confusion matrix, precision and recall. Display the confusion matrix.\n",
    "* Calculate and plot the ROC curve.\n",
    "* Calculate the AUC.\n",
    "\n",
    "You can use tools like ChatGPT to help you write your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0h8BrAMus2HQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0O8F9x1yJOh"
   },
   "source": [
    "**Questions:**\n",
    "* What do each of the components (nodes, branches, thresholds) of the decision tree mean?\n",
    "* Why are there so few points in the ROC curve?  Does it still show useful information?\n",
    "* How does the decision tree compare to the SGD linear model? List 2 pros and 2 cons of each approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hst06Iigs2HQ"
   },
   "source": [
    "## **Model selection [20 mins]**\n",
    "\n",
    "* What do you think would be a good performance metric to use in this case, and why? Choose one to work with here.\n",
    "\n",
    "Note: A good answer here will depend on what you think is most important in the context of the task. If we want to try and supress False Negatives primarily then it would be good to choose an option with a good Recall, but still with acceptable Precision.  Based on the Precision-Recall curves alone you would normally choose the model with an operating point \"nearest\" to the top right. Looking at the class predictions (y_val_pred) as opposed to the probabilities (y_val_prob) shows that it is already choosing a good operating point, as shown also by the confusion matrices.\n",
    "\n",
    "Compare the two models (pipelines) using your chosen performance metric, based on the results from the validation set.\n",
    "\n",
    "Take the chosen model and re-train it on the combination of training and validation datasets.\n",
    "\n",
    "Evaluate the chosen model on the test set. Compare the results to what you got from the validation data.\n",
    "\n",
    "**Question:** What would it mean if there was a big difference between the performance scores on the validation and test datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IF7AnaB-s2HQ"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aRoCrNdzYUU"
   },
   "source": [
    "## **Extension**\n",
    "\n",
    "* How stable do you think these results are?\n",
    "Try flipping the value in one element of a probability or class output (i.e. new_val = 1 - old_val) and see how much the results and curves change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
